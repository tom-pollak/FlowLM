{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kom4xxO0_rIG"
      },
      "source": [
        "# Turning ModernBERT into an instruct-tuned Diffusion LLM\n",
        "\n",
        "An experiment in adapting ModernBERT into a LLADA-style dLLM by fine-tuning it with a variable masking ratio on instruction data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJWVKIYhxcuE"
      },
      "outputs": [],
      "source": [
        "# !pip install -q transformers datasets accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHAi4a3fzr0-"
      },
      "outputs": [],
      "source": [
        "# !pip install -U fsspec datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpVY-zk-TtMG"
      },
      "source": [
        "## Load the model + tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_kxfVoqxg5D"
      },
      "outputs": [],
      "source": [
        "import os, random, itertools, math, torch\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForMaskedLM,\n",
        "    get_cosine_schedule_with_warmup\n",
        ")\n",
        "from torch.optim import AdamW\n",
        "from datasets import load_dataset\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2rEnSuyxi4w",
        "outputId": "720e8326-27db-4116-ec03-c9ccdf9235e1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tokenizer.mask_token='[MASK]'  50284\n"
          ]
        }
      ],
      "source": [
        "model_id = \"answerdotai/ModernBERT-large\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForMaskedLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.bfloat16,          # <-- bfloat16 = cheap & safe\n",
        "    device_map=\"auto\",                   # or set to \"cuda:0\"\n",
        "    low_cpu_mem_usage=True,\n",
        ")\n",
        "mask_id  = tokenizer.mask_token_id\n",
        "cls_id   = tokenizer.cls_token_id\n",
        "sep_id   = tokenizer.sep_token_id\n",
        "\n",
        "print(f\"{tokenizer.mask_token=}  {mask_id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wMILzMsTvNu"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "Including adding masks with rand prob and padding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqtpZDz31KS_",
        "outputId": "408f1431-8f10-4e67-f5d4-7f1d1db1f4a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "raw len: 866137\n"
          ]
        }
      ],
      "source": [
        "raw_ds = load_dataset(\n",
        "    \"allenai/tulu-3-sft-mixture-0225\",\n",
        "    split=\"train\",           # [:1%] % for demo.  drop the slice for real training\n",
        "    cache_dir=\"./data\"\n",
        ")\n",
        "print(\"raw len:\", len(raw_ds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wDhq5SJdxnmI"
      },
      "outputs": [],
      "source": [
        "max_len          = 512\n",
        "mask_ratio_min   = 0.15          # Can tweak\n",
        "mask_ratio_max   = 0.99\n",
        "\n",
        "def join_dialogue(msgs):\n",
        "    \"\"\"\n",
        "    [ {role, content}, ... ]  ->  one flat string with explicit [SEP] boundaries\n",
        "    We expect first msg=user, second msg=assistant.\n",
        "    \"\"\"\n",
        "    u = msgs[0][\"content\"].strip()\n",
        "    a = msgs[1][\"content\"].strip()\n",
        "    return f\"User: {u} {tokenizer.sep_token} Assistant: {a}\"\n",
        "\n",
        "def apply_random_mask(example):\n",
        "    text = join_dialogue(example[\"messages\"])\n",
        "    enc  = tokenizer(text,\n",
        "                     truncation=True, max_length=max_len,\n",
        "                     padding=\"max_length\")\n",
        "    ids  = enc[\"input_ids\"]\n",
        "    labels = [-100] * len(ids)     # -100 -> ignored by CE-loss\n",
        "\n",
        "    # find assistant region (everything after first [SEP])\n",
        "    if sep_id not in ids:\n",
        "        return {**enc, \"labels\": labels}\n",
        "    sep_pos = ids.index(sep_id)         # first [SEP]\n",
        "    cand = [i for i in range(sep_pos+1, len(ids))\n",
        "            if ids[i] not in (tokenizer.pad_token_id,\n",
        "                              cls_id, sep_id)]\n",
        "    if not cand:\n",
        "        return {**enc, \"labels\": labels}\n",
        "\n",
        "    # variable mask ratio\n",
        "    m_ratio = random.uniform(mask_ratio_min, mask_ratio_max)\n",
        "    n_mask  = max(1, int(len(cand) * m_ratio))\n",
        "    chosen  = random.sample(cand, n_mask)\n",
        "\n",
        "    for idx in chosen:\n",
        "        labels[idx] = ids[idx]          # remember ground-truth\n",
        "        dice = random.random()\n",
        "        if dice < 0.8:                  # 80 %\n",
        "            ids[idx] = mask_id\n",
        "        elif dice < 0.9:                # 10 %\n",
        "            ids[idx] = random.randint(0, tokenizer.vocab_size - 1)\n",
        "        # else leave token unchanged (10 %)\n",
        "\n",
        "    enc[\"input_ids\"]    = ids\n",
        "    enc[\"labels\"]       = labels\n",
        "    return enc\n",
        "\n",
        "proc_ds = raw_ds.map(apply_random_mask, remove_columns=raw_ds.column_names, num_proc=32)\n",
        "proc_ds.set_format(type=\"torch\")\n",
        "print(proc_ds[0][\"input_ids\"][:30], \"\\nlabels:\", proc_ds[0][\"labels\"][:30])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "HWcjypby2LhQ",
        "outputId": "98f3f23b-e374-4e29-d933-8eae53a50311"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decoded Input:\n",
            "[CLS]User: A successful businesswoman, Ms. Johnson, donates a portion of her annual earnings to a community fund. This year, she decided to allocate $100,000 of her earnings to create a scholarship program for underprivileged students. Ms. Johnson's earnings follow an exponential growth model due to her flourishing business.\n",
            "\n",
            "1. Ms. Johnson's annual earnings can be modeled by the function \\( E(t) = E_0 e^{kt} \\), where \\( E_0 \\) is her initial earnings, \\( k \\) is the growth rate, and \\( t \\) is the number of years since she started her business. If Ms. Johnson started her business 5 years ago with initial earnings of $150,000, and her current earnings are $450,000, determine the growth rate \\( k \\).\n",
            "\n",
            "2. Based on the growth rate \\( k \\) found in sub-problem 1, predict the amount Ms. Johnson will allocate to the scholarship program in 10 years if she continues to donate the same percentage of her earnings. [SEP] Assistant[MASK][MASK][MASK][MASK][MASK] problem[MASK][MASK][MASK][MASK] perform[MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK] 261[MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK]\n",
            "[MASK][MASK][MASK][MASK][MASK] by:[MASK][MASK][MASK][MASK]([MASK]) =[MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK]] insecurity\n",
            "[MASK]1974[MASK][MASK][MASK][MASK][MASK][MASK] = 150[MASK]000[MASK][MASK][MASK][MASK][MASK][MASK][MASK] Sudan[MASK][MASK][MASK][MASK][MASK] =[MASK][MASK][MASK][MASK]ked[MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK]Rightarrow t[MASK][MASK][MASK][MASK][MASK]\n",
            "[MASK]building[MASK] coolingstra[MASK] growth[MASK][MASK][MASK]iel). Start by[MASK][MASK][MASK][MASK][MASK][MASK] equation Welt[MASK][MASK]\\[[MASK],000[MASK][MASK] minor plaque[MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK] by 150[MASK][MASK][MASK][MASK]ostat[MASK] term:\n",
            "[MASK][MASK][MASK][MASK][MASK][MASK][MASK] =[MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK]000}[MASK][MASK]\n",
            "[MASK][MASK][MASK][MASK][MASK]k[MASK][MASK][MASK][MASK][MASK]\n",
            "\n",
            "To solve[MASK] \\([MASK][MASK]),[MASK] Chelsea[MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK]\\[[MASK][MASK]([MASK][MASK]5 grand[MASK][MASK]+^ Good[MASK][MASK][MASK][MASK]ounded[MASK][MASK][MASK][MASK][MASK] of[MASK][MASK][MASK][MASK][MASK][MASK]ifies[MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK] insecure[MASK]([MASK][MASK][MASK][MASK]\n",
            "[MASK][MASK][MASK] \\( k[MASK][MASK] by dividingDetail[MASK] by同:fecture[MASK][MASK] k[MASK][MASK]cancel[MASK][MASK]([MASK][MASK] disparities[MASK] \\[MASK][SEP]\n",
            "\n",
            "Decoded Labels (Original masked tokens):\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "': To solve the given, we need to perform the following steps:\\n\\n### Part 1: Determine the Growth Rate \\\\( k \\\\)\\n\\nThe earnings function is given:\\n\\n\\\\[ Et) E_0 e^{kt} \\\\]\\n\\nwhere:\\n- \\\\( E_0 = 150, \\\\) (initial earnings)\\n- \\\\( E(t) 450,000 \\\\) (current earnings after 5 years)\\n- \\\\( t = 5 \\\\) years\\nWe need to find the rate \\\\( k \\\\). Start by substituting the known values into the equation:\\n\\n 450,000 = 150,000 e^{5k} \\\\]\\n\\nDivide both sides by 150,000 to isolate the exponential\\n\\\\[ e^{5k} \\\\frac{450,000}{150, \\\\]\\n\\\\[ e^{5k} = 3 \\\\]\\n\\nTo for \\\\( k \\\\ take the natural logarithm of both sides:\\n\\n \\\\ln(e^{k}) = \\\\ln(3) \\\\]\\n\\nBy the properties logarithms, this simplifies to:\\n\\n\\\\[ 5k = \\\\ln3) \\\\]\\n\\nSolve for k \\\\) by both sides 5:\\n\\n\\\\[ = \\\\frac{\\\\ln3)}{5}]'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# prompt: pick a random sample and decode it to visualize the data\n",
        "\n",
        "sample = random.choice(proc_ds)\n",
        "decoded_input = tokenizer.decode(sample[\"input_ids\"], skip_special_tokens=False)\n",
        "print(\"Decoded Input:\")\n",
        "print(decoded_input)\n",
        "\n",
        "# Decode the labels to see the original tokens that were masked\n",
        "labels_to_decode = [label for label in sample[\"labels\"] if label != -100]\n",
        "decoded_labels = tokenizer.decode(labels_to_decode, skip_special_tokens=False)\n",
        "print(\"\\nDecoded Labels (Original masked tokens):\")\n",
        "decoded_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11puVFM9xx6F",
        "outputId": "26d0235b-7c7e-42ad-b921-2676fc0ab40a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input_ids torch.Size([4, 512])\n",
            "attention_mask torch.Size([4, 512])\n",
            "labels torch.Size([4, 512])\n"
          ]
        }
      ],
      "source": [
        "batch = proc_ds[0:4]\n",
        "for k,v in batch.items(): print(k, v.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcePkW5ZTz_U"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROAg0Wpcx1sl"
      },
      "outputs": [],
      "source": [
        "# # A minimal train loop\n",
        "# loader = DataLoader(proc_ds, batch_size=32, shuffle=True)\n",
        "# optim = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
        "# num_epochs      = 1\n",
        "# num_steps       = len(loader)*num_epochs\n",
        "# warmup_steps    = int(0.06 * num_steps)\n",
        "# sched = get_cosine_schedule_with_warmup(optim, warmup_steps, num_steps)\n",
        "# model.train()\n",
        "# for epoch in range(num_epochs):\n",
        "#     for step, batch in enumerate(loader, 1):\n",
        "#         batch = {k:v.to(device) for k,v in batch.items()}\n",
        "#         out   = model(**batch)\n",
        "#         out.loss.backward()\n",
        "#         optim.step(); optim.zero_grad(); sched.step()\n",
        "#         if step % 100 == 0 or step==1:\n",
        "#             print(f\"epoch {epoch} ‖ step {step:4} ‖ loss {out.loss.item():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ShIeezFhAQGU"
      },
      "outputs": [],
      "source": [
        "# ── split 95 % / 5 % after masking ──────────────────────────────────────────────\n",
        "train_ds = proc_ds.shuffle(seed=42).select(range(int(0.95*len(proc_ds))))\n",
        "val_ds   = proc_ds.select(range(int(0.95*len(proc_ds)), len(proc_ds)))\n",
        "\n",
        "batch_size = 32          # ↑ when you have more VRAM\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfhVo69nAVcc"
      },
      "outputs": [],
      "source": [
        "def accuracy_buckets(logits, labels, attn):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "        total_loss   (float, per-token, NOT per-sequence)\n",
        "        global_acc   (float)\n",
        "        bucket_acc   (list[4])  # ≤.25, .25-.5, .5-.75, >.75\n",
        "    \"\"\"\n",
        "    # logits: [B,T,V]  labels: [B,T]\n",
        "    with torch.no_grad():\n",
        "        pred   = logits.argmax(-1)\n",
        "        mask   = labels != -100                  # only masked positions count\n",
        "        correct= (pred == labels) & mask\n",
        "\n",
        "        # ---- global ----\n",
        "        tot_masked = mask.sum().item()\n",
        "        tot_corr   = correct.sum().item()\n",
        "        global_acc = tot_corr / tot_masked if tot_masked else 0.0\n",
        "\n",
        "        # ---- buckets by sample-level mask ratio ----\n",
        "        bucket_corr  = [0,0,0,0]\n",
        "        bucket_total = [0,0,0,0]\n",
        "        edges = (0.25, 0.50, 0.75, 1.01)         # last edge slightly >1\n",
        "\n",
        "        for b in range(labels.size(0)):\n",
        "            n_mask = mask[b].sum().item()\n",
        "            if n_mask == 0:                       # should be rare\n",
        "                continue\n",
        "            # denominator = real tokens (ignore pads)\n",
        "            seq_len = attn[b].sum().item()\n",
        "            ratio   = n_mask / seq_len\n",
        "            # bucket index\n",
        "            for i,edge in enumerate(edges):\n",
        "                if ratio <= edge:\n",
        "                    bucket_total[i]  += n_mask\n",
        "                    bucket_corr[i]   += correct[b].sum().item()\n",
        "                    break\n",
        "\n",
        "        bucket_acc = [c/t if t else 0.0 for c,t in zip(bucket_corr, bucket_total)]\n",
        "    return global_acc, bucket_acc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KdodQ66UAXWk"
      },
      "outputs": [],
      "source": [
        "from itertools import islice\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, val_loader, batches=8):\n",
        "    model.eval()\n",
        "    tot_loss, tot_acc, bucket_hits = 0., 0., [0,0,0,0]\n",
        "    for batch in islice(val_loader, batches):\n",
        "        batch = {k:v.to(device) for k,v in batch.items()}\n",
        "        out   = model(**batch)\n",
        "        loss  = out.loss.item()\n",
        "        tot_loss += loss\n",
        "        acc, bucket_acc = accuracy_buckets(out.logits, batch[\"labels\"], batch[\"attention_mask\"])\n",
        "        tot_acc += acc\n",
        "        bucket_hits = [h+a for h,a in zip(bucket_hits, bucket_acc)]\n",
        "    n = batches\n",
        "    val_loss = tot_loss / n\n",
        "    val_acc  = tot_acc  / n\n",
        "    bucket_acc = [b/n for b in bucket_hits]\n",
        "    return val_loss, val_acc, bucket_acc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DmueLvORAY6s"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "optim = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
        "num_epochs           = 1\n",
        "steps_per_epoch      = len(train_loader)\n",
        "total_steps          = num_epochs * steps_per_epoch\n",
        "warmup_steps         = int(0.06 * total_steps)\n",
        "sched = get_cosine_schedule_with_warmup(optim, warmup_steps, total_steps)\n",
        "\n",
        "log_every   = 200                 # validate every n updates\n",
        "global_step = 0\n",
        "losses, val_losses = [], []\n",
        "accs, val_accs = [], []\n",
        "\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    pbar = tqdm(train_loader, total=steps_per_epoch,\n",
        "                desc=f\"Epoch {epoch}\", leave=False, dynamic_ncols=True)\n",
        "\n",
        "    running_loss = 0.0\n",
        "    running_acc  = 0.0\n",
        "    for step, batch in enumerate(pbar, 1):\n",
        "        global_step += 1\n",
        "        batch = {k:v.to(device) for k,v in batch.items()}\n",
        "        out   = model(**batch)\n",
        "        out.loss.backward()\n",
        "\n",
        "        optim.step(); optim.zero_grad(); sched.step()\n",
        "\n",
        "        # ── on-the-fly training accuracy ───────────────────────────────────────\n",
        "        acc, _ = accuracy_buckets(out.logits.detach(), batch[\"labels\"], batch[\"attention_mask\"])\n",
        "        running_loss += out.loss.item()\n",
        "        running_acc  += acc\n",
        "\n",
        "        losses.append(out.loss.item())\n",
        "        accs.append(acc)\n",
        "\n",
        "        # progress bar message\n",
        "        if step % 20 == 0 or step == 1:\n",
        "            pbar.set_postfix(loss = running_loss/step,\n",
        "                             acc  = running_acc / step)\n",
        "\n",
        "        # ── periodic validation ────────────────────────────────────────────────\n",
        "        if global_step % log_every == 0:\n",
        "            val_loss, val_acc, val_buckets = evaluate(model, val_loader)\n",
        "            val_losses.append(val_loss)\n",
        "            val_accs.append(val_acc)\n",
        "            print(f\"\\n🧮 step {global_step:6d} | \"\n",
        "                  f\"train_loss {running_loss/step:.4f}  \"\n",
        "                  f\"train_acc {running_acc/step:.3f} | \"\n",
        "                  f\"val_loss {val_loss:.4f}  val_acc {val_acc:.3f} | \"\n",
        "                  f\"bucket_acc {['{:.3f}'.format(x) for x in val_buckets]}\\n\")\n",
        "            model.train()               # back to train-mode\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yoglbi9K34RM"
      },
      "source": [
        "- Should I scale loss based on masking ratio?\n",
        "- What's the length distribution in the dataset? Is 512 too big?\n",
        "- Can I do larger batch size?\n",
        "- Fixed batch or 10 for eval with fixed masking ratio for smooth val loss curve\n",
        "- Full dataset run\n",
        "- Mask ratio schedule so we warm up to full masking?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWCZlZSA4Z2n",
        "outputId": "510bf4f0-b2ff-45de-a536-48706e111215"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('modernbert-diffusion-finetuned/tokenizer_config.json',\n",
              " 'modernbert-diffusion-finetuned/special_tokens_map.json',\n",
              " 'modernbert-diffusion-finetuned/tokenizer.json')"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "save_dir = \"modernbert-diffusion-finetuned\"\n",
        "model.save_pretrained(save_dir)\n",
        "tokenizer.save_pretrained(save_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iX657LU4D7Kl"
      },
      "outputs": [],
      "source": [
        "# repo_id = \"johnowhitaker/modernbert-diffusion\"\n",
        "# model.push_to_hub(\n",
        "#     repo_id,\n",
        "#     token=\"my_token\",           # or pass your string literal\n",
        "#     private=True,                          # drop this if the repo can be public\n",
        "#     commit_message=\"Add diffusion-style fine-tuned weights\",\n",
        "# )\n",
        "# tokenizer.push_to_hub(repo_id, token=\"my_token\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EP4oLf4oEqu5"
      },
      "outputs": [],
      "source": [
        "# prompt: plot the train and val losses and accuracies (two separate subplots)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plotting the loss\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(losses, label='Train Loss')\n",
        "# Since val_losses are recorded less frequently, we need to align the x-axis.\n",
        "# We log every `log_every` steps.\n",
        "val_x = [i * log_every for i in range(1, len(val_losses) + 1)]\n",
        "plt.plot(val_x, val_losses, label='Validation Loss')\n",
        "plt.xlabel('Training Steps')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Train and Validation Loss over Steps')\n",
        "plt.legend()\n",
        "\n",
        "# Plotting the accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(accs, label='Train Accuracy')\n",
        "# Aligning val_accs with the corresponding steps\n",
        "plt.plot(val_x, val_accs, label='Validation Accuracy')\n",
        "plt.xlabel('Training Steps')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Train and Validation Accuracy over Steps')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
